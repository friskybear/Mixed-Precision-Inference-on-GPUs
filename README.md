# Mixed-Precision Inference on GPUs

> A GPU computing benchmark that runs a two-layer neural network across **6 different precision strategies** simultaneously, measuring speed, memory, and accuracy in real time.

Built with **Rust Â· OpenCL Â· CLBlast Â· Tauri Â· React Â· TypeScript**

---

## What This Project Does

This tool runs the same neural network computation six different ways â€” each using a different numerical precision or library â€” and displays live charts comparing:

- **Execution Time** (ms)
- **Throughput** (GFLOPS)
- **Memory Bandwidth** (GB/s)
- **Accuracy** (Mean Squared Error vs FP32 baseline)

All six modes use the **same random input and deterministic weights** each round, so the comparison is always fair.

---

## The Six Precision Modes

| # | Mode | Kernel | Precision | Notes |
|---|------|--------|-----------|-------|
| 1 | ğŸŸ¢ **FP32** | Custom OpenCL | 32-bit float | Gold standard reference |
| 2 | ğŸ”µ **FP16** | Custom OpenCL | 16-bit float | ~50% memory, some accuracy loss |
| 3 | ğŸŸ£ **FP16 + Scale** | Custom OpenCL | FP16 weights + FP32 scales | Per-row quantization for better accuracy |
| 4 | ğŸŸ¡ **CLBlast FP32** | BLAS SGEMM | 32-bit float | Optimized matrix multiply |
| 5 | ğŸ©µ **CLBlast FP16** | BLAS HGEMM | 16-bit float | Optimized half-precision BLAS |
| 6 | ğŸ©· **CLBlast Mixed** | BLAS SGEMM | FP16 stored â†’ FP32 compute | Best of both worlds |

---

## Quick Start

### Prerequisites

- OpenCL-capable GPU (NVIDIA, AMD, or Intel) with up-to-date drivers
- [Rust](https://rustup.rs/) 1.70+
- [Node.js](https://nodejs.org/) 18+ **or** [Bun](https://bun.sh/)

### Run in Development

```bash
# Install frontend dependencies
npm install        # or: bun install

# Start the app (compiles Rust + launches Vite + opens the window)
npm run tauri dev  # or: bun tauri dev
```

The first launch is slower â€” OpenCL kernels compile and CLBlast auto-tunes for the selected matrix size. Subsequent launches at the same matrix size are fast.

### Build for Release

```bash
npm run tauri build
```

The output is a self-contained installer in `src-tauri/target/release/bundle/`. The CLBlast DLL is embedded inside the binary â€” no separate installation needed.

---

## Using the App

1. **Select a matrix size** with the slider: `128 Â· 256 Â· 512 Â· 1024`
   - This sets `input_size = hidden_size = matrix_size`, `output_size = matrix_size / 2`
   - Larger sizes show bigger performance differences between modes
2. **Press â–¶ Play** to start continuous benchmarking
3. **Watch the charts** update live â€” each iteration runs all six modes back-to-back
4. **Press â¹ Stop** at any time
5. Changing the matrix size resets the charts automatically

> **Tip:** The very first round after launching (or after changing matrix size) is slower than subsequent rounds because CLBlast runs its internal auto-tuner. This is expected and normal.

---

## Automatic Logging & Plotting

Every **5 rounds**, the app automatically saves a complete snapshot of all metrics collected so far:

| File | Contents |
|------|----------|
| `metrics.csv` | All metrics from round 1 to current, all 6 modes, all fields |
| `execution_time.png` | Line chart comparing execution time across all modes |
| `throughput.png` | GFLOPS comparison |
| `bandwidth.png` | Memory bandwidth comparison |
| `accuracy_mse.png` | Accuracy MSE vs FP32 baseline (FP16, FP16+Scale, CLBlast FP16, CLBlast Mixed) |

**Log path format:**
```
parallel_log/{YYYY-MM-DD_HH-MM-SS}_{matrix_size}/
â”œâ”€â”€ metrics.csv
â”œâ”€â”€ execution_time.png
â”œâ”€â”€ throughput.png
â”œâ”€â”€ bandwidth.png
â””â”€â”€ accuracy_mse.png
```

- A **new session folder** is created each time logging triggers (timestamped + matrix size).
- Changing the matrix size resets the metrics accumulator, so the next session starts fresh.
- Charts are generated server-side in Rust using the **plotters** crate â€” no browser or external tools needed.
- Colors in the PNG charts match the live dashboard (green/blue/purple/yellow/cyan/pink).

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            React + TypeScript             â”‚
â”‚  Live charts Â· Metrics panel Â· Controls  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  Tauri IPC (invoke / JSON)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Rust Backend (lib.rs)           â”‚
â”‚  run_comparison_inference()              â”‚
â”‚  run_inference()  Â·  get_len()           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MLPInference (types.rs) â”‚
        â”‚   Singleton Â· GPU state   â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ OpenCL      â”‚  â”‚  CLBlast (DLL)   â”‚
     â”‚ Custom      â”‚  â”‚  SGEMM / HGEMM   â”‚
     â”‚ Kernels     â”‚  â”‚  embedded in .exeâ”‚
     â”‚ (kernel.rs) â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend â€” Rust (`src-tauri/src/`)

| File | Responsibility |
|------|----------------|
| `lib.rs` | Tauri commands, DLL loading, device selection, float helpers |
| `types.rs` | All structs + all six inference functions (the heavy math) |
| `kernel.rs` | OpenCL C kernel source code (compiled at runtime by the GPU driver) |
| `logger.rs` | CSV export + PNG chart generation (plotters) for automatic logging |
| `main.rs` | Minimal entry point â€” calls `lib::run()` |

**Key design decisions:**

- **Singleton GPU context** â€” `MLP_INSTANCE: Mutex<Option<MLPInference>>` holds one OpenCL context for the entire session, preventing repeated initialization and CLBlast re-tuning.
- **Embedded DLL** â€” `clblast.dll` is baked into the binary via `include_bytes!` and extracted to a temp folder on first run. Zero external dependencies.
- **Two-pass kernel design** â€” Layer 1 and Layer 2 are separate kernel dispatches with a `queue.finish()` barrier between them. This ensures each hidden neuron is computed exactly once, not redundantly per output neuron.
- **Pre-compiled helper kernels** â€” the bias/ReLU and FP16â†”FP32 conversion kernels are compiled once at startup and reused across all rounds.

### Frontend â€” React + TypeScript (`src/`)

| File | Responsibility |
|------|----------------|
| `App.tsx` | Main UI â€” state management, inference loop, chart data, layout |
| `component/chart.tsx` | Reusable Highcharts area chart wrapper |

The inference loop is a self-chaining async function that calls `invoke("run_comparison_inference")`, dispatches a single `PUSH` action to update all chart series in one React re-render, yields to the browser for a frame, then loops.

---

## Implementation Details

### The Neural Network Workload

A two-layer MLP (Multi-Layer Perceptron):

```
Input  [batch=64 Ã— input_size]
  â†“  Ã— weights1áµ€ [hidden_size Ã— input_size]
  â†“  + bias1  â†’  ReLU
Hidden [batch=64 Ã— hidden_size]
  â†“  Ã— weights2áµ€ [output_size Ã— hidden_size]
  â†“  + bias2
Output [batch=64 Ã— output_size]
```

- `batch_size` is fixed at **64**
- `input_size = hidden_size = matrix_size` (from slider)
- `output_size = matrix_size / 2`

### Row-Wise Scaling (Mode 3)

Before converting weights to FP16, each row is normalized so its maximum absolute value is 1.0:

```
scale[h]        = max(|weights1[h][:]|)
weights_fp16[h] = weights1[h] / scale[h]   â† stored in GPU as FP16
```

During the kernel, the scale is re-applied to recover the original magnitude:

```c
sum += (float)input[i] * (float)weights_fp16[h][i] * scale[h];
```

This keeps all FP16 values in `[-1.0, 1.0]` â€” the range where FP16 has the most precision. The hidden buffer is kept as FP32 to avoid a second round of quantization error.

### CLBlast Integration (Modes 4â€“6)

CLBlast's GEMM functions handle the matrix multiplication. Because BLAS doesn't know about neural network biases or activation functions, small helper OpenCL kernels handle those steps:

- `add_bias_relu_fp32/fp16` â€” adds bias and applies `ReLU` after Layer 1
- `add_bias_fp32/fp16` â€” adds bias only (no activation) after Layer 2

The CLBlast Mixed mode additionally runs `convert_fp16_to_fp32` kernels on the GPU to expand FP16-stored weights into FP32 buffers before passing them to SGEMM.

---

## Performance Metrics

All metrics are calculated per inference run:

**Throughput (GFLOPS)**
```
total_flops = batch Ã— [hidden Ã— (2Ã—input + 1) + output Ã— (2Ã—hidden + 1)]
throughput  = total_flops / time_seconds / 1e9
```

**Memory Bandwidth (GB/s)**
```
bytes_transferred = Î£ (bytes read + bytes written) per kernel
bandwidth         = bytes_transferred / time_seconds / 1e9
```
FP16 buffers count 2 bytes/element; FP32 buffers count 4 bytes/element.

**Accuracy (MSE)**
```
mse = Î£ (output[i] - fp32_reference[i])Â² / N
```
FP32 and CLBlast FP32 always report MSE = 0.0 â€” they are the reference or compute equivalent precision.

---

## Expected Results

Performance varies significantly by GPU. These are approximate relative values at `matrix_size = 512`:

| Mode | Memory Footprint | Relative Throughput | Accuracy MSE |
|------|-----------------|---------------------|--------------|
| FP32 | 100% (baseline) | 1.0Ã— | 0.0 (reference) |
| FP16 | ~50% | 1.2 â€“ 1.8Ã— | Small, non-zero |
| FP16 + Scale | ~52% | 1.0 â€“ 1.5Ã— | Better than FP16 |
| CLBlast FP32 | ~100% | 3 â€“ 10Ã— | 0.0 |
| CLBlast FP16 | ~50% | 4 â€“ 15Ã— | Similar to FP16 |
| CLBlast Mixed | ~150% | 2 â€“ 8Ã— | Very close to 0.0 |

> CLBlast advantages grow at larger matrix sizes. At `matrix_size = 128`, the custom kernels may be competitive or faster due to CLBlast overhead.

---

## Project Structure

```
Mixed-Precision-Inference-on-GPUs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.tsx                  # Main UI component and inference loop
â”‚   â”œâ”€â”€ component/
â”‚   â”‚   â””â”€â”€ chart.tsx            # Reusable Highcharts area chart
â”‚   â””â”€â”€ main.tsx                 # React entry point
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs               # Tauri commands, DLL loading, device selection
â”‚   â”‚   â”œâ”€â”€ types.rs             # Structs + all six inference functions
â”‚   â”‚   â”œâ”€â”€ kernel.rs            # OpenCL C kernel source (as string constants)
â”‚   â”‚   â”œâ”€â”€ logger.rs            # CSV + PNG chart generation (plotters crate)
â”‚   â”‚   â””â”€â”€ main.rs              # Binary entry point
â”‚   â”œâ”€â”€ clblast.dll              # CLBlast library (embedded into binary at compile time)
â”‚   â”œâ”€â”€ Cargo.toml               # Rust dependencies
â”‚   â””â”€â”€ tauri.conf.json          # Tauri app configuration
â”œâ”€â”€ parallel_log/                # Auto-generated logs (created at runtime)
â”‚   â””â”€â”€ {timestamp}_{size}/      # One folder per logging session
â”‚       â”œâ”€â”€ metrics.csv
â”‚       â”œâ”€â”€ execution_time.png
â”‚       â”œâ”€â”€ throughput.png
â”‚       â”œâ”€â”€ bandwidth.png
â”‚       â””â”€â”€ accuracy_mse.png
â”œâ”€â”€ presentation/
â”‚   â””â”€â”€ presentation.tex         # LaTeX Beamer presentation
â”œâ”€â”€ CODE_EXPLANATION.md          # Deep dive into every file, struct, and function
â”œâ”€â”€ project.md                   # Project presentation (objectives, results, analysis)
â”œâ”€â”€ package.json                 # Node dependencies
â””â”€â”€ vite.config.ts               # Vite build configuration
```

---

## Dependencies

### Rust

| Crate | Version | Purpose |
|-------|---------|---------|
| `tauri` | 2.x | Desktop app framework |
| `opencl3` | 0.12 | OpenCL bindings |
| `libloading` | 0.9 | Dynamic DLL loading |
| `half` | 2.x | FP16 â†” FP32 conversion |
| `rand` | 0.10 | Random input generation |
| `serde` | 1.x | JSON serialization for Tauri IPC |
| `plotters` | 0.3 | PNG chart generation for automatic logging |
| `chrono` | 0.4 | Timestamps for log folder names |

### Node / Frontend

| Package | Purpose |
|---------|---------|
| `react` 19 | UI framework |
| `highcharts` + `highcharts-react-official` | Live area charts |
| `@tauri-apps/api` | `invoke()` bridge to Rust |
| `tailwindcss` + `daisyui` | Styling |
| `typescript` 5.8 | Type safety |
| `vite` 7 | Build tool |

---

## Troubleshooting

**No OpenCL devices found**
Install GPU drivers that include OpenCL support. For NVIDIA: CUDA Toolkit or GeForce drivers. For AMD: ROCm or Adrenalin drivers. For Intel: Intel oneAPI runtime.

**CLBlast modes show 0.0 / fail silently**
Some older GPUs don't support FP16 BLAS operations (HGEMM). The CLBlast FP16 and Mixed modes fall back gracefully to zero metrics instead of crashing.

**First round is very slow**
Expected. OpenCL kernels compile at runtime (~100â€“500ms) and CLBlast auto-tunes on the first GEMM call for each matrix size (~1â€“3 seconds). All subsequent rounds are fast.

**Build fails on Windows**
```bash
cargo clean
npm run tauri build
```
Ensure the Visual Studio C++ build tools are installed (required for Rust on Windows).

**Performance seems low / CPU is being used instead of GPU**
Check that your GPU's OpenCL runtime is installed. The app logs the selected device type at startup. CPU OpenCL is correct but much slower than a GPU.

---

## Further Reading

- [`CODE_EXPLANATION.md`](./CODE_EXPLANATION.md) â€” Complete explanation of every concept, struct, and function in the codebase, from first principles (what is a batch? what is GEMM? what is a kernel?)
- [`project.md`](./project.md) â€” Full project presentation covering objectives, implementation, results, and conclusions

---

## License

Academic project for GPU computing coursework.

---

*Built with* **Rust ğŸ¦€ Â· Tauri ğŸš€ Â· React âš›ï¸ Â· OpenCL Â· CLBlast**